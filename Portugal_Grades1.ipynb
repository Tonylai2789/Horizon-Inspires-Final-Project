{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "import optuna #hyperparameter optimization\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor, StackingRegressor\n",
    "from sklearn.model_selection import RandomizedSearchCV, cross_val_predict\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, make_scorer, r2_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'student-por.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X = df[['G1', 'G2']]\n",
    "X = pd.concat([X, df.drop(columns=['G3'])], axis=1) #include all other features except G3\n",
    "y = df[['G3']] #g3 is target var\n",
    "\n",
    "#one hot encoding for categorical variables\n",
    "X = pd.get_dummies(X, drop_first=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove Outliers \n",
    "(Using IQR to remove outliers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from scipy import stats\n",
    "import numpy as np \n",
    "\n",
    "#only complete task on numerical columns\n",
    "numerical_columns = X.select_dtypes(include=[np.number])\n",
    "\n",
    "#calculate q1 and q1\n",
    "Q1 = numerical_columns.quantile(0.25)\n",
    "Q3 = numerical_columns.quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "#outlier bounds\n",
    "lower_bound = numerical_columns < (Q1 - 1.5 * IQR)\n",
    "upper_bound = numerical_columns > (Q3 + 1.5 * IQR)\n",
    "\n",
    "\n",
    "outliers = lower_bound | upper_bound\n",
    "non_outliers = ~outliers.any(axis=1)\n",
    "\n",
    "#remove outliers\n",
    "X_clean = X[non_outliers]\n",
    "y_clean = y[non_outliers]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attempt with RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_clean, y_clean, test_size=0.2, random_state=42)\n",
    "\n",
    "y_train = y_train.squeeze()\n",
    "y_test = y_test.squeeze()\n",
    "#make sure features are in correct format\n",
    "X_train = X_train.values if isinstance(X_train, pd.DataFrame) else X_train\n",
    "X_test = X_test.values if isinstance(X_test, pd.DataFrame) else X_test\n",
    "\n",
    "#define XG boost model\n",
    "xgb_model = xgb.XGBRegressor(objective='reg:squarederror', random_state=42)\n",
    "\n",
    "#define parameter grid\n",
    "#specifies range of hyperparams that will be searched through\n",
    "#RSCV --> randomly samples fixed # of combinations, evaluates useing cross-val\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300], # num. trees in ensemble\n",
    "    'max_depth': [3, 4, 5], #max depth of tree\n",
    "    'learning_rate': [0.1, 0.2], #how much model adjusts each round\n",
    "    'subsample': [0.7, 0.8], #fraction of training data used to build tree\n",
    "    'colsample_bytree': [0.7, 0.8], #fraction of features selected for each tree\n",
    "    'reg_alpha': [0.5, 1, 2], #l1 reg.\n",
    "    'reg_lambda': [1.5, 2, 3], #l2 reg.\n",
    "}\n",
    "\n",
    "#set up scoring functions\n",
    "scorers = {\n",
    "    'mse': make_scorer(mean_squared_error, greater_is_better=False),\n",
    "    'mae': make_scorer(mean_absolute_error, greater_is_better=False)\n",
    "}\n",
    "\n",
    "#set up RandomizedSearchCV for mse\n",
    "random_search_mse = RandomizedSearchCV(estimator=xgb_model, param_distributions=param_grid, \n",
    "                                   n_iter=100, cv=5, verbose=2, random_state=42, \n",
    "                                   scoring='neg_mean_squared_error')\n",
    "\n",
    "random_search_mse.fit(X_train, y_train.values.ravel())\n",
    "\n",
    "#setup RandomizedSearchCV for mae\n",
    "random_search_mae = RandomizedSearchCV(estimator=xgb_model, param_distributions=param_grid, \n",
    "                                       n_iter=100, cv=5, verbose=2, random_state=42, \n",
    "                                       scoring=scorers['mae'])\n",
    "random_search_mae.fit(X_train, y_train)\n",
    "\n",
    "#eval on test(validation) set\n",
    "y_pred_mse = random_search_mse.best_estimator_.predict(X_test)\n",
    "y_pred_mae = random_search_mae.best_estimator_.predict(X_test)\n",
    "\n",
    "mse_final = mean_squared_error(y_test, y_pred_mse)\n",
    "mae_final = mean_absolute_error(y_test, y_pred_mae)\n",
    "print(f'XGBoost Model Mean Squared Error: {mse_final}')\n",
    "print(f'XGBoost Model Mean ABS Error: {mae_final}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With RandomizedSearchCV, MSE --> 1.62, MAE --> 0.74"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After removing outliers, MSE 1.36, MAE 0.85"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attempt with Optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-08-12 16:04:39,519] A new study created in memory with name: no-name-77871733-90cf-41b3-b8aa-08f5482819ba\n",
      "[I 2024-08-12 16:04:51,001] Trial 0 finished with value: 1.259661946396306 and parameters: {'n_estimators': 501, 'max_depth': 17, 'learning_rate': 0.29744836946008013, 'subsample': 0.6656843344048304, 'colsample_bytree': 0.8040149492631081, 'reg_alpha': 9.084053418259344, 'reg_lambda': 7.795201045695941, 'min_child_weight': 1, 'gamma': 0.04363189360224229}. Best is trial 0 with value: 1.259661946396306.\n",
      "[I 2024-08-12 16:04:57,858] Trial 1 finished with value: 1.121434037019144 and parameters: {'n_estimators': 568, 'max_depth': 10, 'learning_rate': 0.2708570465436004, 'subsample': 0.8601015995181187, 'colsample_bytree': 0.504277562654605, 'reg_alpha': 7.162313953473563, 'reg_lambda': 7.058843042801874, 'min_child_weight': 3, 'gamma': 2.5964599382130538}. Best is trial 1 with value: 1.121434037019144.\n",
      "[I 2024-08-12 16:05:06,936] Trial 2 finished with value: 1.1352938459806157 and parameters: {'n_estimators': 782, 'max_depth': 19, 'learning_rate': 0.3519381251870127, 'subsample': 0.8293295605188489, 'colsample_bytree': 0.6187803212708642, 'reg_alpha': 8.415011663907451, 'reg_lambda': 5.740289047638589, 'min_child_weight': 7, 'gamma': 2.4575749673714045}. Best is trial 1 with value: 1.121434037019144.\n",
      "[I 2024-08-12 16:05:18,395] Trial 3 finished with value: 1.1127913822159599 and parameters: {'n_estimators': 966, 'max_depth': 16, 'learning_rate': 0.16115156956233204, 'subsample': 0.7119146055594636, 'colsample_bytree': 0.8329639262623789, 'reg_alpha': 8.169961459935644, 'reg_lambda': 9.729555310347052, 'min_child_weight': 3, 'gamma': 2.898188130032594}. Best is trial 3 with value: 1.1127913822159599.\n",
      "[I 2024-08-12 16:05:25,448] Trial 4 finished with value: 1.1309517495724983 and parameters: {'n_estimators': 428, 'max_depth': 4, 'learning_rate': 0.037391238381380004, 'subsample': 0.9832149129018872, 'colsample_bytree': 0.9548483368348248, 'reg_alpha': 5.673679923963142, 'reg_lambda': 1.8010824205867404, 'min_child_weight': 2, 'gamma': 4.837957114893985}. Best is trial 3 with value: 1.1127913822159599.\n",
      "[I 2024-08-12 16:05:37,045] Trial 5 finished with value: 1.1867094541136574 and parameters: {'n_estimators': 489, 'max_depth': 7, 'learning_rate': 0.20480274074847402, 'subsample': 0.7847550052676329, 'colsample_bytree': 0.803863204245336, 'reg_alpha': 9.511000476785853, 'reg_lambda': 9.219978062505774, 'min_child_weight': 4, 'gamma': 0.0401252275531494}. Best is trial 3 with value: 1.1127913822159599.\n",
      "[I 2024-08-12 16:05:39,566] Trial 6 finished with value: 1.137463697934534 and parameters: {'n_estimators': 182, 'max_depth': 4, 'learning_rate': 0.2030158000377094, 'subsample': 0.6136249214520988, 'colsample_bytree': 0.8729441251535188, 'reg_alpha': 7.346340474173262, 'reg_lambda': 5.908650103448956, 'min_child_weight': 8, 'gamma': 4.619980257359206}. Best is trial 3 with value: 1.1127913822159599.\n",
      "[I 2024-08-12 16:05:42,304] Trial 7 finished with value: 1.0649822988102895 and parameters: {'n_estimators': 192, 'max_depth': 13, 'learning_rate': 0.229764306413671, 'subsample': 0.640938850161547, 'colsample_bytree': 0.9249281687502472, 'reg_alpha': 5.5659725105036, 'reg_lambda': 9.504540643145932, 'min_child_weight': 5, 'gamma': 2.9787698879307998}. Best is trial 7 with value: 1.0649822988102895.\n",
      "[I 2024-08-12 16:05:51,517] Trial 8 finished with value: 1.1081381476896233 and parameters: {'n_estimators': 534, 'max_depth': 7, 'learning_rate': 0.06885533318017087, 'subsample': 0.5487950556981245, 'colsample_bytree': 0.9882342023991944, 'reg_alpha': 1.2529339287415464, 'reg_lambda': 9.825641494276569, 'min_child_weight': 8, 'gamma': 2.4343558962806884}. Best is trial 7 with value: 1.0649822988102895.\n",
      "[I 2024-08-12 16:06:00,338] Trial 9 finished with value: 1.0931936630385917 and parameters: {'n_estimators': 710, 'max_depth': 4, 'learning_rate': 0.15862045570313063, 'subsample': 0.6162146847892417, 'colsample_bytree': 0.9121555902920491, 'reg_alpha': 0.3853203910093417, 'reg_lambda': 2.468643236644037, 'min_child_weight': 5, 'gamma': 4.9696380320272135}. Best is trial 7 with value: 1.0649822988102895.\n",
      "[I 2024-08-12 16:06:01,961] Trial 10 finished with value: 1.2611366269120157 and parameters: {'n_estimators': 112, 'max_depth': 13, 'learning_rate': 0.42715939779368284, 'subsample': 0.5603613784353723, 'colsample_bytree': 0.7000589207503461, 'reg_alpha': 3.1299533337022107, 'reg_lambda': 3.872634135886071, 'min_child_weight': 10, 'gamma': 3.8255061917957107}. Best is trial 7 with value: 1.0649822988102895.\n",
      "[I 2024-08-12 16:06:12,575] Trial 11 finished with value: 1.1477419109001372 and parameters: {'n_estimators': 761, 'max_depth': 12, 'learning_rate': 0.12474445318355765, 'subsample': 0.5113807047437373, 'colsample_bytree': 0.916307682438557, 'reg_alpha': 4.416174050486985, 'reg_lambda': 0.23285549415386875, 'min_child_weight': 5, 'gamma': 1.258503696681093}. Best is trial 7 with value: 1.0649822988102895.\n",
      "[I 2024-08-12 16:06:17,446] Trial 12 finished with value: 1.0592676814677258 and parameters: {'n_estimators': 311, 'max_depth': 10, 'learning_rate': 0.1134950343410756, 'subsample': 0.6394019466399434, 'colsample_bytree': 0.9995334305179813, 'reg_alpha': 0.2853680028016626, 'reg_lambda': 3.520354638597718, 'min_child_weight': 6, 'gamma': 3.798040968179545}. Best is trial 12 with value: 1.0592676814677258.\n",
      "[I 2024-08-12 16:06:22,206] Trial 13 finished with value: 1.0593915075900335 and parameters: {'n_estimators': 288, 'max_depth': 9, 'learning_rate': 0.08316313877327763, 'subsample': 0.7088638904799279, 'colsample_bytree': 0.9900480971814016, 'reg_alpha': 2.345631716046918, 'reg_lambda': 4.015359615456128, 'min_child_weight': 6, 'gamma': 3.5215328079723993}. Best is trial 12 with value: 1.0592676814677258.\n",
      "[I 2024-08-12 16:06:27,352] Trial 14 finished with value: 1.0556105088517878 and parameters: {'n_estimators': 319, 'max_depth': 8, 'learning_rate': 0.08042813030626086, 'subsample': 0.7176104449240441, 'colsample_bytree': 0.9908765994870746, 'reg_alpha': 2.163252198816903, 'reg_lambda': 3.9426293339714436, 'min_child_weight': 7, 'gamma': 3.7363444780500035}. Best is trial 14 with value: 1.0556105088517878.\n",
      "[I 2024-08-12 16:06:39,178] Trial 15 finished with value: 1.1367346268962057 and parameters: {'n_estimators': 345, 'max_depth': 8, 'learning_rate': 0.011322554624937961, 'subsample': 0.7606562838459922, 'colsample_bytree': 0.7181005463598921, 'reg_alpha': 0.13875633083639494, 'reg_lambda': 3.1435558880020356, 'min_child_weight': 10, 'gamma': 4.058190013378452}. Best is trial 14 with value: 1.0556105088517878.\n",
      "[I 2024-08-12 16:06:44,859] Trial 16 finished with value: 1.0680191818337803 and parameters: {'n_estimators': 357, 'max_depth': 11, 'learning_rate': 0.10491077016636503, 'subsample': 0.8939932205665662, 'colsample_bytree': 0.8642889169031001, 'reg_alpha': 2.074788517810582, 'reg_lambda': 1.3726468367750488, 'min_child_weight': 7, 'gamma': 1.7151938383858094}. Best is trial 14 with value: 1.0556105088517878.\n",
      "[I 2024-08-12 16:06:48,345] Trial 17 finished with value: 1.063427045903635 and parameters: {'n_estimators': 283, 'max_depth': 6, 'learning_rate': 0.4552543733924086, 'subsample': 0.6963439550372025, 'colsample_bytree': 0.9992542781252972, 'reg_alpha': 3.497119763603682, 'reg_lambda': 4.615288521197751, 'min_child_weight': 8, 'gamma': 4.230970241836654}. Best is trial 14 with value: 1.0556105088517878.\n",
      "[I 2024-08-12 16:06:56,545] Trial 18 finished with value: 1.0595607393511792 and parameters: {'n_estimators': 632, 'max_depth': 10, 'learning_rate': 0.14568421246811236, 'subsample': 0.7840373010006381, 'colsample_bytree': 0.6342664981966696, 'reg_alpha': 1.1857770035106907, 'reg_lambda': 5.8553647471403725, 'min_child_weight': 6, 'gamma': 3.153762441752445}. Best is trial 14 with value: 1.0556105088517878.\n",
      "[I 2024-08-12 16:07:03,072] Trial 19 finished with value: 1.0693279088976444 and parameters: {'n_estimators': 390, 'max_depth': 15, 'learning_rate': 0.053702585352860197, 'subsample': 0.5828410137802538, 'colsample_bytree': 0.7646226992667995, 'reg_alpha': 4.021037129427209, 'reg_lambda': 3.3221121728819156, 'min_child_weight': 7, 'gamma': 3.5733647672502604}. Best is trial 14 with value: 1.0556105088517878.\n",
      "[I 2024-08-12 16:07:06,536] Trial 20 finished with value: 1.2114921015746727 and parameters: {'n_estimators': 234, 'max_depth': 14, 'learning_rate': 0.3267643924036356, 'subsample': 0.6697664258848102, 'colsample_bytree': 0.8752382394392747, 'reg_alpha': 2.4269967538243895, 'reg_lambda': 0.6298432580031674, 'min_child_weight': 9, 'gamma': 1.8416839342434643}. Best is trial 14 with value: 1.0556105088517878.\n",
      "[I 2024-08-12 16:07:11,308] Trial 21 finished with value: 1.0677600229427544 and parameters: {'n_estimators': 282, 'max_depth': 9, 'learning_rate': 0.09238786484496997, 'subsample': 0.7165620711425577, 'colsample_bytree': 0.9571912066594175, 'reg_alpha': 1.6432033256974508, 'reg_lambda': 4.6794075660042544, 'min_child_weight': 6, 'gamma': 3.674643734550562}. Best is trial 14 with value: 1.0556105088517878.\n",
      "[I 2024-08-12 16:07:15,937] Trial 22 finished with value: 1.9866500346563676 and parameters: {'n_estimators': 116, 'max_depth': 6, 'learning_rate': 0.0104288412952252, 'subsample': 0.7263493665926206, 'colsample_bytree': 0.9932194206846447, 'reg_alpha': 2.695180203836976, 'reg_lambda': 4.260659789615875, 'min_child_weight': 6, 'gamma': 3.3710567487931606}. Best is trial 14 with value: 1.0556105088517878.\n",
      "[I 2024-08-12 16:07:20,913] Trial 23 finished with value: 1.0633324854598765 and parameters: {'n_estimators': 298, 'max_depth': 9, 'learning_rate': 0.08127818919332412, 'subsample': 0.6682432944378022, 'colsample_bytree': 0.9439300364899574, 'reg_alpha': 0.7847559738041965, 'reg_lambda': 2.4723634988032166, 'min_child_weight': 4, 'gamma': 4.392879728271419}. Best is trial 14 with value: 1.0556105088517878.\n",
      "[I 2024-08-12 16:07:26,894] Trial 24 finished with value: 1.0796753899558653 and parameters: {'n_estimators': 439, 'max_depth': 11, 'learning_rate': 0.18145195761611838, 'subsample': 0.8125678119487547, 'colsample_bytree': 0.9654782753117324, 'reg_alpha': 0.03587902139964311, 'reg_lambda': 3.6190581041054037, 'min_child_weight': 7, 'gamma': 3.985340520228294}. Best is trial 14 with value: 1.0556105088517878.\n",
      "[I 2024-08-12 16:07:30,550] Trial 25 finished with value: 1.05430871676503 and parameters: {'n_estimators': 210, 'max_depth': 8, 'learning_rate': 0.11553908728748671, 'subsample': 0.759943245046274, 'colsample_bytree': 0.9114759969481703, 'reg_alpha': 1.853701295452605, 'reg_lambda': 5.179795593197564, 'min_child_weight': 9, 'gamma': 3.383451045319588}. Best is trial 25 with value: 1.05430871676503.\n",
      "[I 2024-08-12 16:07:33,665] Trial 26 finished with value: 1.0522789933047696 and parameters: {'n_estimators': 191, 'max_depth': 6, 'learning_rate': 0.13250964039436516, 'subsample': 0.7544239450662534, 'colsample_bytree': 0.9034581525111515, 'reg_alpha': 1.6129544113079195, 'reg_lambda': 5.2185859306727655, 'min_child_weight': 9, 'gamma': 4.431198313995694}. Best is trial 26 with value: 1.0522789933047696.\n",
      "[I 2024-08-12 16:07:36,501] Trial 27 finished with value: 1.1019949471346298 and parameters: {'n_estimators': 212, 'max_depth': 3, 'learning_rate': 0.24572450797977347, 'subsample': 0.9199801936940937, 'colsample_bytree': 0.8972154028202612, 'reg_alpha': 4.714014244852956, 'reg_lambda': 5.281886169862021, 'min_child_weight': 9, 'gamma': 4.448602253623169}. Best is trial 26 with value: 1.0522789933047696.\n",
      "[I 2024-08-12 16:07:39,317] Trial 28 finished with value: 1.0547348572585244 and parameters: {'n_estimators': 161, 'max_depth': 6, 'learning_rate': 0.14085172849921956, 'subsample': 0.7677824456772443, 'colsample_bytree': 0.8272475164075297, 'reg_alpha': 3.538132415938706, 'reg_lambda': 7.032268189200186, 'min_child_weight': 9, 'gamma': 3.2294245312493723}. Best is trial 26 with value: 1.0522789933047696.\n",
      "[I 2024-08-12 16:07:41,981] Trial 29 finished with value: 1.0631501091854016 and parameters: {'n_estimators': 149, 'max_depth': 5, 'learning_rate': 0.13713516878069684, 'subsample': 0.7586876975317943, 'colsample_bytree': 0.8213051390882335, 'reg_alpha': 3.373149699957231, 'reg_lambda': 7.018598401980442, 'min_child_weight': 9, 'gamma': 3.218916041300884}. Best is trial 26 with value: 1.0522789933047696.\n",
      "[I 2024-08-12 16:07:44,624] Trial 30 finished with value: 1.0918948917338653 and parameters: {'n_estimators': 104, 'max_depth': 6, 'learning_rate': 0.1974670341018035, 'subsample': 0.8300518485915919, 'colsample_bytree': 0.7841175581874572, 'reg_alpha': 5.268897104985376, 'reg_lambda': 8.259473861464208, 'min_child_weight': 10, 'gamma': 0.6612318976095699}. Best is trial 26 with value: 1.0522789933047696.\n",
      "[I 2024-08-12 16:07:49,811] Trial 31 finished with value: 1.0654423903902357 and parameters: {'n_estimators': 235, 'max_depth': 7, 'learning_rate': 0.04564152752950414, 'subsample': 0.7430588760491509, 'colsample_bytree': 0.8416836250618999, 'reg_alpha': 1.6923665290461298, 'reg_lambda': 6.771089198926666, 'min_child_weight': 9, 'gamma': 4.163361882463288}. Best is trial 26 with value: 1.0522789933047696.\n",
      "[I 2024-08-12 16:07:51,742] Trial 32 finished with value: 1.0580509121840236 and parameters: {'n_estimators': 169, 'max_depth': 8, 'learning_rate': 0.2930462142150648, 'subsample': 0.7891715969527923, 'colsample_bytree': 0.8885543373755052, 'reg_alpha': 2.8564960283658833, 'reg_lambda': 6.313961661814885, 'min_child_weight': 8, 'gamma': 3.0062834583110174}. Best is trial 26 with value: 1.0522789933047696.\n",
      "[I 2024-08-12 16:07:54,957] Trial 33 finished with value: 1.0515063830804485 and parameters: {'n_estimators': 257, 'max_depth': 8, 'learning_rate': 0.12084785289859168, 'subsample': 0.8607535895103247, 'colsample_bytree': 0.5048763603131963, 'reg_alpha': 3.935303505304648, 'reg_lambda': 7.959633133233437, 'min_child_weight': 9, 'gamma': 2.560632467444421}. Best is trial 33 with value: 1.0515063830804485.\n",
      "[I 2024-08-12 16:08:04,699] Trial 34 finished with value: 1.064078883089429 and parameters: {'n_estimators': 986, 'max_depth': 20, 'learning_rate': 0.17033511297291762, 'subsample': 0.8809932985012321, 'colsample_bytree': 0.5044498034269893, 'reg_alpha': 4.080386496935709, 'reg_lambda': 8.120941212673262, 'min_child_weight': 10, 'gamma': 2.6611804458162034}. Best is trial 33 with value: 1.0515063830804485.\n",
      "[I 2024-08-12 16:08:07,759] Trial 35 finished with value: 1.0823374955815412 and parameters: {'n_estimators': 240, 'max_depth': 5, 'learning_rate': 0.11378195240214047, 'subsample': 0.9393463284169628, 'colsample_bytree': 0.5470398096500024, 'reg_alpha': 6.139855675886196, 'reg_lambda': 7.415100336757105, 'min_child_weight': 9, 'gamma': 2.344950591334218}. Best is trial 33 with value: 1.0515063830804485.\n",
      "[I 2024-08-12 16:08:11,563] Trial 36 finished with value: 1.065310744514154 and parameters: {'n_estimators': 391, 'max_depth': 3, 'learning_rate': 0.22567523010150947, 'subsample': 0.8228407167710042, 'colsample_bytree': 0.6379049349613574, 'reg_alpha': 6.464232741110108, 'reg_lambda': 5.203887109918023, 'min_child_weight': 1, 'gamma': 2.7342985782599976}. Best is trial 33 with value: 1.0515063830804485.\n",
      "[I 2024-08-12 16:08:21,636] Trial 37 finished with value: 1.06054974003248 and parameters: {'n_estimators': 900, 'max_depth': 5, 'learning_rate': 0.2842548409762684, 'subsample': 0.8525967833926692, 'colsample_bytree': 0.5865544328967753, 'reg_alpha': 4.043214761236222, 'reg_lambda': 8.867370597244065, 'min_child_weight': 8, 'gamma': 2.1319594652673928}. Best is trial 33 with value: 1.0515063830804485.\n",
      "[I 2024-08-12 16:08:26,558] Trial 38 finished with value: 1.0958733389671775 and parameters: {'n_estimators': 458, 'max_depth': 18, 'learning_rate': 0.3690121131863516, 'subsample': 0.9685450203372782, 'colsample_bytree': 0.7288499091267097, 'reg_alpha': 3.5444317975091924, 'reg_lambda': 6.497205589247982, 'min_child_weight': 9, 'gamma': 4.6910663093889715}. Best is trial 33 with value: 1.0515063830804485.\n",
      "[I 2024-08-12 16:08:28,881] Trial 39 finished with value: 1.086215829215488 and parameters: {'n_estimators': 162, 'max_depth': 7, 'learning_rate': 0.13950506525699283, 'subsample': 0.7806024180929426, 'colsample_bytree': 0.8119068485188199, 'reg_alpha': 4.960097734266449, 'reg_lambda': 7.584743100596769, 'min_child_weight': 10, 'gamma': 2.086332120532546}. Best is trial 33 with value: 1.0515063830804485.\n",
      "[I 2024-08-12 16:08:35,576] Trial 40 finished with value: 1.0733721554811264 and parameters: {'n_estimators': 603, 'max_depth': 6, 'learning_rate': 0.18041556661560584, 'subsample': 0.8631982652417719, 'colsample_bytree': 0.8529834506005547, 'reg_alpha': 0.921306105348206, 'reg_lambda': 8.750880080972525, 'min_child_weight': 8, 'gamma': 1.4469365777756278}. Best is trial 33 with value: 1.0515063830804485.\n",
      "[I 2024-08-12 16:08:39,010] Trial 41 finished with value: 1.0435413021802193 and parameters: {'n_estimators': 234, 'max_depth': 8, 'learning_rate': 0.06985591259394475, 'subsample': 0.7470832041749002, 'colsample_bytree': 0.9277813303192752, 'reg_alpha': 1.920517530918925, 'reg_lambda': 5.502907339812075, 'min_child_weight': 7, 'gamma': 3.358724828406568}. Best is trial 41 with value: 1.0435413021802193.\n",
      "[I 2024-08-12 16:08:42,923] Trial 42 finished with value: 1.0457733203806978 and parameters: {'n_estimators': 255, 'max_depth': 8, 'learning_rate': 0.05864761845563493, 'subsample': 0.8009542344550069, 'colsample_bytree': 0.6735213472162002, 'reg_alpha': 1.469762865729328, 'reg_lambda': 5.716927858514966, 'min_child_weight': 9, 'gamma': 2.871923620566464}. Best is trial 41 with value: 1.0435413021802193.\n",
      "[I 2024-08-12 16:08:47,198] Trial 43 finished with value: 1.0476359289426918 and parameters: {'n_estimators': 203, 'max_depth': 10, 'learning_rate': 0.0383334592071717, 'subsample': 0.8047681547661489, 'colsample_bytree': 0.6790781206999, 'reg_alpha': 1.8671707472628947, 'reg_lambda': 5.283075283248865, 'min_child_weight': 8, 'gamma': 2.7642303977794027}. Best is trial 41 with value: 1.0435413021802193.\n",
      "[I 2024-08-12 16:08:51,406] Trial 44 finished with value: 1.037821936657524 and parameters: {'n_estimators': 251, 'max_depth': 10, 'learning_rate': 0.055401550395562174, 'subsample': 0.800207543245092, 'colsample_bytree': 0.6814527591792401, 'reg_alpha': 1.43356182676701, 'reg_lambda': 6.15169700989261, 'min_child_weight': 8, 'gamma': 2.8162171700829184}. Best is trial 44 with value: 1.037821936657524.\n",
      "[I 2024-08-12 16:08:56,657] Trial 45 finished with value: 1.046894048281507 and parameters: {'n_estimators': 249, 'max_depth': 10, 'learning_rate': 0.03468614319981307, 'subsample': 0.8068073631945487, 'colsample_bytree': 0.6771559420283708, 'reg_alpha': 1.0250938428308909, 'reg_lambda': 6.17474625282052, 'min_child_weight': 7, 'gamma': 2.768218009104356}. Best is trial 44 with value: 1.037821936657524.\n",
      "[I 2024-08-12 16:09:02,863] Trial 46 finished with value: 1.0417345324807112 and parameters: {'n_estimators': 367, 'max_depth': 12, 'learning_rate': 0.03628989969989735, 'subsample': 0.7939174585718093, 'colsample_bytree': 0.6745136339926545, 'reg_alpha': 0.8795215997553039, 'reg_lambda': 6.250164322904356, 'min_child_weight': 7, 'gamma': 2.8796809667867294}. Best is trial 44 with value: 1.037821936657524.\n",
      "[I 2024-08-12 16:09:08,148] Trial 47 finished with value: 1.050129307176976 and parameters: {'n_estimators': 372, 'max_depth': 12, 'learning_rate': 0.0602898769755245, 'subsample': 0.8405903842536213, 'colsample_bytree': 0.6725421240617445, 'reg_alpha': 0.8146557683484936, 'reg_lambda': 6.069630575817355, 'min_child_weight': 7, 'gamma': 2.8796380848215795}. Best is trial 44 with value: 1.037821936657524.\n",
      "[I 2024-08-12 16:09:17,298] Trial 48 finished with value: 1.0496699804673377 and parameters: {'n_estimators': 534, 'max_depth': 13, 'learning_rate': 0.03114096017292378, 'subsample': 0.8076851971146772, 'colsample_bytree': 0.6578522328176907, 'reg_alpha': 1.152537961774362, 'reg_lambda': 5.709046823386775, 'min_child_weight': 7, 'gamma': 2.3502651792497544}. Best is trial 44 with value: 1.037821936657524.\n",
      "[I 2024-08-12 16:09:22,998] Trial 49 finished with value: 1.1516878472274048 and parameters: {'n_estimators': 261, 'max_depth': 11, 'learning_rate': 0.02838269502580557, 'subsample': 0.7928896745921852, 'colsample_bytree': 0.6036088016064286, 'reg_alpha': 8.713182231223698, 'reg_lambda': 5.667388233955566, 'min_child_weight': 4, 'gamma': 3.0175749313630287}. Best is trial 44 with value: 1.037821936657524.\n",
      "[I 2024-08-12 16:09:28,656] Trial 50 finished with value: 1.1349231310920487 and parameters: {'n_estimators': 341, 'max_depth': 12, 'learning_rate': 0.05622957419306679, 'subsample': 0.7352324171950388, 'colsample_bytree': 0.7000332874903827, 'reg_alpha': 9.981466030404047, 'reg_lambda': 4.60707582583031, 'min_child_weight': 5, 'gamma': 2.232754679397617}. Best is trial 44 with value: 1.037821936657524.\n",
      "[I 2024-08-12 16:09:36,674] Trial 51 finished with value: 1.040751692191002 and parameters: {'n_estimators': 322, 'max_depth': 10, 'learning_rate': 0.030950908434472832, 'subsample': 0.8090681259819748, 'colsample_bytree': 0.6778386682038015, 'reg_alpha': 1.4668651510713366, 'reg_lambda': 6.414009849134231, 'min_child_weight': 8, 'gamma': 2.7460558021144443}. Best is trial 44 with value: 1.037821936657524.\n",
      "[I 2024-08-12 16:09:44,558] Trial 52 finished with value: 1.0270544948589564 and parameters: {'n_estimators': 477, 'max_depth': 9, 'learning_rate': 0.06949498623442997, 'subsample': 0.8335748304636379, 'colsample_bytree': 0.7460596794356147, 'reg_alpha': 0.4876962436118415, 'reg_lambda': 6.362934169133188, 'min_child_weight': 7, 'gamma': 2.502967212761555}. Best is trial 52 with value: 1.0270544948589564.\n",
      "[I 2024-08-12 16:09:53,152] Trial 53 finished with value: 1.0449166249844173 and parameters: {'n_estimators': 494, 'max_depth': 13, 'learning_rate': 0.07102844680090394, 'subsample': 0.8828089033673283, 'colsample_bytree': 0.7376150308225262, 'reg_alpha': 0.4610217298103587, 'reg_lambda': 6.315754628021426, 'min_child_weight': 8, 'gamma': 1.8183424798530745}. Best is trial 52 with value: 1.0270544948589564.\n",
      "[I 2024-08-12 16:10:00,889] Trial 54 finished with value: 1.0549049802784283 and parameters: {'n_estimators': 475, 'max_depth': 14, 'learning_rate': 0.0966110798395024, 'subsample': 0.8417247755894809, 'colsample_bytree': 0.7522305107428149, 'reg_alpha': 0.5675630301257718, 'reg_lambda': 6.685276081043387, 'min_child_weight': 8, 'gamma': 1.864529449481339}. Best is trial 52 with value: 1.0270544948589564.\n",
      "[I 2024-08-12 16:10:10,106] Trial 55 finished with value: 1.062015267906018 and parameters: {'n_estimators': 416, 'max_depth': 14, 'learning_rate': 0.07366553153704258, 'subsample': 0.8944666680252528, 'colsample_bytree': 0.7188717329704796, 'reg_alpha': 0.2809933729281955, 'reg_lambda': 7.335598703197802, 'min_child_weight': 6, 'gamma': 1.0088594277034026}. Best is trial 52 with value: 1.0270544948589564.\n",
      "[I 2024-08-12 16:10:29,258] Trial 56 finished with value: 1.0481213949391452 and parameters: {'n_estimators': 509, 'max_depth': 13, 'learning_rate': 0.01005721176643308, 'subsample': 0.880293410770272, 'colsample_bytree': 0.7929105597264915, 'reg_alpha': 0.6748263151180511, 'reg_lambda': 6.498297241527007, 'min_child_weight': 7, 'gamma': 2.467521168609292}. Best is trial 52 with value: 1.0270544948589564.\n",
      "[I 2024-08-12 16:10:38,434] Trial 57 finished with value: 1.0804753578707273 and parameters: {'n_estimators': 557, 'max_depth': 11, 'learning_rate': 0.07613232718887701, 'subsample': 0.6904763651784681, 'colsample_bytree': 0.7399647452564371, 'reg_alpha': 2.566454557490465, 'reg_lambda': 6.9094343236358435, 'min_child_weight': 8, 'gamma': 1.4728453854357135}. Best is trial 52 with value: 1.0270544948589564.\n",
      "[I 2024-08-12 16:10:46,597] Trial 58 finished with value: 1.045963893761618 and parameters: {'n_estimators': 610, 'max_depth': 16, 'learning_rate': 0.09338911278955292, 'subsample': 0.9154697255969937, 'colsample_bytree': 0.6989514818085516, 'reg_alpha': 0.4581283477366471, 'reg_lambda': 4.761022203854847, 'min_child_weight': 7, 'gamma': 3.392313006075833}. Best is trial 52 with value: 1.0270544948589564.\n",
      "[I 2024-08-12 16:10:53,338] Trial 59 finished with value: 1.0510973545786215 and parameters: {'n_estimators': 336, 'max_depth': 9, 'learning_rate': 0.06678053447826893, 'subsample': 0.8268462415197569, 'colsample_bytree': 0.7725164904518708, 'reg_alpha': 1.2619500136433435, 'reg_lambda': 5.546153606522936, 'min_child_weight': 6, 'gamma': 1.9162674452649298}. Best is trial 52 with value: 1.0270544948589564.\n",
      "[I 2024-08-12 16:11:05,039] Trial 60 finished with value: 1.0395538146802645 and parameters: {'n_estimators': 405, 'max_depth': 10, 'learning_rate': 0.02290459290007913, 'subsample': 0.8698371585510581, 'colsample_bytree': 0.6447287461997305, 'reg_alpha': 0.03610915858528557, 'reg_lambda': 6.08083092194192, 'min_child_weight': 5, 'gamma': 2.586045706974017}. Best is trial 52 with value: 1.0270544948589564.\n",
      "[I 2024-08-12 16:11:16,777] Trial 61 finished with value: 1.034994890565962 and parameters: {'n_estimators': 422, 'max_depth': 10, 'learning_rate': 0.024425966690449687, 'subsample': 0.8709346723344514, 'colsample_bytree': 0.6497166337472201, 'reg_alpha': 0.0983040975136561, 'reg_lambda': 6.11142598711733, 'min_child_weight': 5, 'gamma': 2.555356739607994}. Best is trial 52 with value: 1.0270544948589564.\n",
      "[I 2024-08-12 16:11:29,034] Trial 62 finished with value: 1.0373554103123204 and parameters: {'n_estimators': 416, 'max_depth': 10, 'learning_rate': 0.022735621168382472, 'subsample': 0.7741035917078176, 'colsample_bytree': 0.6353152360943921, 'reg_alpha': 0.13832977923465448, 'reg_lambda': 6.011542239771215, 'min_child_weight': 5, 'gamma': 2.5707699333989793}. Best is trial 52 with value: 1.0270544948589564.\n",
      "[I 2024-08-12 16:11:41,318] Trial 63 finished with value: 1.0442866203280667 and parameters: {'n_estimators': 419, 'max_depth': 10, 'learning_rate': 0.02276652674642867, 'subsample': 0.8404144966405945, 'colsample_bytree': 0.6428268366477875, 'reg_alpha': 0.2766183761952168, 'reg_lambda': 5.9116834650513574, 'min_child_weight': 5, 'gamma': 2.559383424201688}. Best is trial 52 with value: 1.0270544948589564.\n",
      "[I 2024-08-12 16:11:49,581] Trial 64 finished with value: 1.0374428254712782 and parameters: {'n_estimators': 381, 'max_depth': 9, 'learning_rate': 0.046457830335572436, 'subsample': 0.7751479903359932, 'colsample_bytree': 0.6112339348231277, 'reg_alpha': 0.005537838870403086, 'reg_lambda': 7.233905151174183, 'min_child_weight': 3, 'gamma': 3.0620308572273673}. Best is trial 52 with value: 1.0270544948589564.\n",
      "[I 2024-08-12 16:12:05,678] Trial 65 finished with value: 1.0272948144491496 and parameters: {'n_estimators': 445, 'max_depth': 9, 'learning_rate': 0.01762461122420092, 'subsample': 0.7760586999511742, 'colsample_bytree': 0.5851584774482488, 'reg_alpha': 0.019357632086241167, 'reg_lambda': 7.263686791297587, 'min_child_weight': 3, 'gamma': 2.209706022199958}. Best is trial 52 with value: 1.0270544948589564.\n",
      "[I 2024-08-12 16:12:20,782] Trial 66 finished with value: 1.0258953537913578 and parameters: {'n_estimators': 446, 'max_depth': 9, 'learning_rate': 0.02067935889374753, 'subsample': 0.7671397873594592, 'colsample_bytree': 0.575340248735303, 'reg_alpha': 0.0892268000458934, 'reg_lambda': 7.557538897218079, 'min_child_weight': 3, 'gamma': 2.1333590586988485}. Best is trial 66 with value: 1.0258953537913578.\n",
      "[I 2024-08-12 16:12:39,543] Trial 67 finished with value: 1.0383224252422085 and parameters: {'n_estimators': 445, 'max_depth': 9, 'learning_rate': 0.011816565285881668, 'subsample': 0.7727229193929613, 'colsample_bytree': 0.5590646027873384, 'reg_alpha': 0.026918291725330334, 'reg_lambda': 7.789827760774278, 'min_child_weight': 3, 'gamma': 2.0428472519431966}. Best is trial 66 with value: 1.0258953537913578.\n",
      "[I 2024-08-12 16:12:48,571] Trial 68 finished with value: 1.1169167840400323 and parameters: {'n_estimators': 509, 'max_depth': 9, 'learning_rate': 0.04665016924232404, 'subsample': 0.6998214033159684, 'colsample_bytree': 0.6148868162950156, 'reg_alpha': 7.526239426373217, 'reg_lambda': 8.525949982642272, 'min_child_weight': 3, 'gamma': 2.3939354026496082}. Best is trial 66 with value: 1.0258953537913578.\n",
      "[I 2024-08-12 16:12:56,993] Trial 69 finished with value: 1.1779144416654 and parameters: {'n_estimators': 666, 'max_depth': 11, 'learning_rate': 0.49218204310927227, 'subsample': 0.7282069355490725, 'colsample_bytree': 0.5845354655947184, 'reg_alpha': 0.44782870220944104, 'reg_lambda': 7.22129074422762, 'min_child_weight': 2, 'gamma': 3.1043793636658434}. Best is trial 66 with value: 1.0258953537913578.\n",
      "[I 2024-08-12 16:13:08,071] Trial 70 finished with value: 1.077129044266678 and parameters: {'n_estimators': 471, 'max_depth': 9, 'learning_rate': 0.046603176099318015, 'subsample': 0.7843528608096287, 'colsample_bytree': 0.5594661191028009, 'reg_alpha': 0.6008177478659653, 'reg_lambda': 7.719472499078206, 'min_child_weight': 2, 'gamma': 1.6944193257812876}. Best is trial 66 with value: 1.0258953537913578.\n",
      "[I 2024-08-12 16:13:27,116] Trial 71 finished with value: 1.0454254473469522 and parameters: {'n_estimators': 448, 'max_depth': 7, 'learning_rate': 0.011409146544093455, 'subsample': 0.7716990391565546, 'colsample_bytree': 0.5363160118503133, 'reg_alpha': 0.02312819912753139, 'reg_lambda': 8.339761581580778, 'min_child_weight': 3, 'gamma': 2.0018588880636576}. Best is trial 66 with value: 1.0258953537913578.\n",
      "[I 2024-08-12 16:13:41,944] Trial 72 finished with value: 1.0279223057461873 and parameters: {'n_estimators': 444, 'max_depth': 9, 'learning_rate': 0.019742419787465385, 'subsample': 0.7705338421349692, 'colsample_bytree': 0.5719294017697438, 'reg_alpha': 0.017898329238654753, 'reg_lambda': 9.227730480185366, 'min_child_weight': 4, 'gamma': 2.1981057022793054}. Best is trial 66 with value: 1.0258953537913578.\n",
      "[I 2024-08-12 16:13:51,480] Trial 73 finished with value: 1.0330688178460885 and parameters: {'n_estimators': 528, 'max_depth': 9, 'learning_rate': 0.05057558457823725, 'subsample': 0.8196719429641242, 'colsample_bytree': 0.5865280293708779, 'reg_alpha': 0.3308055179438453, 'reg_lambda': 9.960374194023077, 'min_child_weight': 4, 'gamma': 2.1531052362279826}. Best is trial 66 with value: 1.0258953537913578.\n",
      "[I 2024-08-12 16:13:58,930] Trial 74 finished with value: 1.033876000404876 and parameters: {'n_estimators': 581, 'max_depth': 9, 'learning_rate': 0.10349652326258778, 'subsample': 0.7487743229828733, 'colsample_bytree': 0.5901122550200251, 'reg_alpha': 0.2938600963865714, 'reg_lambda': 9.590023405516508, 'min_child_weight': 4, 'gamma': 2.274870186347923}. Best is trial 66 with value: 1.0258953537913578.\n",
      "[I 2024-08-12 16:14:06,149] Trial 75 finished with value: 1.0463513843703038 and parameters: {'n_estimators': 589, 'max_depth': 8, 'learning_rate': 0.08956741866656487, 'subsample': 0.8221652240258267, 'colsample_bytree': 0.5906064280773486, 'reg_alpha': 1.036624372279059, 'reg_lambda': 9.469379497221453, 'min_child_weight': 4, 'gamma': 2.174933690367431}. Best is trial 66 with value: 1.0258953537913578.\n",
      "[I 2024-08-12 16:14:18,355] Trial 76 finished with value: 1.0305565745423562 and parameters: {'n_estimators': 543, 'max_depth': 7, 'learning_rate': 0.01861648912646542, 'subsample': 0.7492936896962876, 'colsample_bytree': 0.5247306510300437, 'reg_alpha': 0.29275226977143387, 'reg_lambda': 9.993831008646996, 'min_child_weight': 4, 'gamma': 2.2498896536958126}. Best is trial 66 with value: 1.0258953537913578.\n",
      "[I 2024-08-12 16:14:26,517] Trial 77 finished with value: 1.0486099901511008 and parameters: {'n_estimators': 660, 'max_depth': 7, 'learning_rate': 0.10262170742712283, 'subsample': 0.7356199220902032, 'colsample_bytree': 0.571461387064714, 'reg_alpha': 0.3285287326411327, 'reg_lambda': 9.993467272353486, 'min_child_weight': 4, 'gamma': 2.274396392364506}. Best is trial 66 with value: 1.0258953537913578.\n",
      "[I 2024-08-12 16:14:32,528] Trial 78 finished with value: 1.1477486213307648 and parameters: {'n_estimators': 540, 'max_depth': 9, 'learning_rate': 0.31655328076936196, 'subsample': 0.7544874490632818, 'colsample_bytree': 0.5255616079254181, 'reg_alpha': 0.7634057866963385, 'reg_lambda': 9.40625933908939, 'min_child_weight': 4, 'gamma': 1.752851940563933}. Best is trial 66 with value: 1.0258953537913578.\n",
      "[I 2024-08-12 16:14:41,087] Trial 79 finished with value: 1.1458806017851095 and parameters: {'n_estimators': 572, 'max_depth': 8, 'learning_rate': 0.08548664605351569, 'subsample': 0.5007422886952466, 'colsample_bytree': 0.5285174188210194, 'reg_alpha': 1.2481913382395198, 'reg_lambda': 9.135411349168832, 'min_child_weight': 4, 'gamma': 1.5924938668454047}. Best is trial 66 with value: 1.0258953537913578.\n",
      "[I 2024-08-12 16:14:49,294] Trial 80 finished with value: 1.0331451661375293 and parameters: {'n_estimators': 506, 'max_depth': 7, 'learning_rate': 0.04484683745034123, 'subsample': 0.71509615407605, 'colsample_bytree': 0.573235537866541, 'reg_alpha': 2.2182949543030412, 'reg_lambda': 9.204585711109253, 'min_child_weight': 3, 'gamma': 2.0149206022810713}. Best is trial 66 with value: 1.0258953537913578.\n",
      "[I 2024-08-12 16:14:58,312] Trial 81 finished with value: 1.0385344906300866 and parameters: {'n_estimators': 498, 'max_depth': 7, 'learning_rate': 0.044299349432716434, 'subsample': 0.7139095870348018, 'colsample_bytree': 0.5709548781976516, 'reg_alpha': 0.6374472525705643, 'reg_lambda': 9.777079234949502, 'min_child_weight': 3, 'gamma': 2.014765060434442}. Best is trial 66 with value: 1.0258953537913578.\n",
      "[I 2024-08-12 16:15:09,023] Trial 82 finished with value: 1.0387300545308022 and parameters: {'n_estimators': 511, 'max_depth': 9, 'learning_rate': 0.024260222409143297, 'subsample': 0.7445461460055155, 'colsample_bytree': 0.5970888931874603, 'reg_alpha': 0.2914642304001686, 'reg_lambda': 9.004994080018434, 'min_child_weight': 4, 'gamma': 2.263900823640658}. Best is trial 66 with value: 1.0258953537913578.\n",
      "[I 2024-08-12 16:15:14,795] Trial 83 finished with value: 1.1406413714760433 and parameters: {'n_estimators': 528, 'max_depth': 8, 'learning_rate': 0.37754691592619083, 'subsample': 0.6851503942822129, 'colsample_bytree': 0.5734101757099938, 'reg_alpha': 0.8233996144959401, 'reg_lambda': 9.616723605004854, 'min_child_weight': 2, 'gamma': 2.4189174047503528}. Best is trial 66 with value: 1.0258953537913578.\n",
      "[I 2024-08-12 16:15:22,881] Trial 84 finished with value: 1.0481771221097556 and parameters: {'n_estimators': 569, 'max_depth': 11, 'learning_rate': 0.060866891445098735, 'subsample': 0.72179540858741, 'colsample_bytree': 0.6225683224705353, 'reg_alpha': 2.1794933561983783, 'reg_lambda': 8.611957165453452, 'min_child_weight': 4, 'gamma': 2.1283054452155934}. Best is trial 66 with value: 1.0258953537913578.\n",
      "[I 2024-08-12 16:15:31,084] Trial 85 finished with value: 1.0442224064503018 and parameters: {'n_estimators': 467, 'max_depth': 7, 'learning_rate': 0.049802417365388846, 'subsample': 0.7615064082088792, 'colsample_bytree': 0.5479265247068258, 'reg_alpha': 0.49173605444291757, 'reg_lambda': 9.330486075295141, 'min_child_weight': 3, 'gamma': 1.9517610591626229}. Best is trial 66 with value: 1.0258953537913578.\n",
      "[I 2024-08-12 16:15:42,605] Trial 86 finished with value: 1.029704905749668 and parameters: {'n_estimators': 438, 'max_depth': 8, 'learning_rate': 0.021439273562899954, 'subsample': 0.8520128618649238, 'colsample_bytree': 0.5186370703943893, 'reg_alpha': 1.0859508034204721, 'reg_lambda': 9.92491271930604, 'min_child_weight': 4, 'gamma': 1.5754035566903946}. Best is trial 66 with value: 1.0258953537913578.\n",
      "[I 2024-08-12 16:15:54,806] Trial 87 finished with value: 1.0723644553596352 and parameters: {'n_estimators': 623, 'max_depth': 5, 'learning_rate': 0.03865401824043974, 'subsample': 0.6476247143520921, 'colsample_bytree': 0.5236286694084269, 'reg_alpha': 1.0251724475291826, 'reg_lambda': 9.857983935798805, 'min_child_weight': 3, 'gamma': 1.2271275443843983}. Best is trial 66 with value: 1.0258953537913578.\n",
      "[I 2024-08-12 16:16:03,971] Trial 88 finished with value: 1.0776941160093108 and parameters: {'n_estimators': 582, 'max_depth': 7, 'learning_rate': 0.10549992385040698, 'subsample': 0.70369325373354, 'colsample_bytree': 0.5174048567519748, 'reg_alpha': 1.3353017712800015, 'reg_lambda': 9.128785639208106, 'min_child_weight': 4, 'gamma': 1.6356611976883073}. Best is trial 66 with value: 1.0258953537913578.\n",
      "[I 2024-08-12 16:16:13,809] Trial 89 finished with value: 1.0742508799428092 and parameters: {'n_estimators': 650, 'max_depth': 6, 'learning_rate': 0.08236005301459706, 'subsample': 0.6768483903607061, 'colsample_bytree': 0.5417170506444506, 'reg_alpha': 1.5649019030881512, 'reg_lambda': 9.635697822775766, 'min_child_weight': 2, 'gamma': 1.545192707635183}. Best is trial 66 with value: 1.0258953537913578.\n",
      "[I 2024-08-12 16:16:16,707] Trial 90 finished with value: 1.0487026959809052 and parameters: {'n_estimators': 701, 'max_depth': 8, 'learning_rate': 0.06299831890290249, 'subsample': 0.8167973275653118, 'colsample_bytree': 0.5136613451058095, 'reg_alpha': 0.6827472565874486, 'reg_lambda': 9.986889154789726, 'min_child_weight': 3, 'gamma': 1.210928247212976}. Best is trial 66 with value: 1.0258953537913578.\n",
      "[I 2024-08-12 16:16:19,585] Trial 91 finished with value: 1.0281678609959437 and parameters: {'n_estimators': 483, 'max_depth': 10, 'learning_rate': 0.02305388539087934, 'subsample': 0.8528966792432526, 'colsample_bytree': 0.5541106027481403, 'reg_alpha': 0.25179563391060633, 'reg_lambda': 8.984308699543515, 'min_child_weight': 5, 'gamma': 2.286533570582976}. Best is trial 66 with value: 1.0258953537913578.\n",
      "[I 2024-08-12 16:16:23,160] Trial 92 finished with value: 1.0290428474960334 and parameters: {'n_estimators': 474, 'max_depth': 9, 'learning_rate': 0.017977584908466432, 'subsample': 0.8510464772107387, 'colsample_bytree': 0.5624200039751687, 'reg_alpha': 0.42611777592869016, 'reg_lambda': 8.844831976722185, 'min_child_weight': 4, 'gamma': 2.2283274766374865}. Best is trial 66 with value: 1.0258953537913578.\n",
      "[I 2024-08-12 16:16:27,114] Trial 93 finished with value: 1.0363210686264557 and parameters: {'n_estimators': 482, 'max_depth': 8, 'learning_rate': 0.01653962987237817, 'subsample': 0.8483630717574888, 'colsample_bytree': 0.5581382810543124, 'reg_alpha': 1.0637908402675142, 'reg_lambda': 8.851316690030561, 'min_child_weight': 4, 'gamma': 2.1586993373838306}. Best is trial 66 with value: 1.0258953537913578.\n",
      "[I 2024-08-12 16:16:30,348] Trial 94 finished with value: 1.0230013178475659 and parameters: {'n_estimators': 521, 'max_depth': 9, 'learning_rate': 0.03656342583573152, 'subsample': 0.8538407322597121, 'colsample_bytree': 0.5797937964609392, 'reg_alpha': 0.9027189587273957, 'reg_lambda': 8.038524588813868, 'min_child_weight': 5, 'gamma': 1.848075949374476}. Best is trial 94 with value: 1.0230013178475659.\n",
      "[I 2024-08-12 16:16:33,698] Trial 95 finished with value: 1.0326334265124193 and parameters: {'n_estimators': 450, 'max_depth': 9, 'learning_rate': 0.03064815556990834, 'subsample': 0.8338268726149317, 'colsample_bytree': 0.501508049358903, 'reg_alpha': 0.8141425922454045, 'reg_lambda': 8.062622361906447, 'min_child_weight': 5, 'gamma': 1.335358742011628}. Best is trial 94 with value: 1.0230013178475659.\n",
      "[I 2024-08-12 16:16:37,023] Trial 96 finished with value: 1.0425173336425355 and parameters: {'n_estimators': 438, 'max_depth': 10, 'learning_rate': 0.03356814792754526, 'subsample': 0.8346073869459745, 'colsample_bytree': 0.50010833055527, 'reg_alpha': 1.7701327337480792, 'reg_lambda': 8.005166107909957, 'min_child_weight': 5, 'gamma': 1.0890498477567894}. Best is trial 94 with value: 1.0230013178475659.\n",
      "[I 2024-08-12 16:16:41,453] Trial 97 finished with value: 1.034500978296301 and parameters: {'n_estimators': 397, 'max_depth': 8, 'learning_rate': 0.02077728355453684, 'subsample': 0.898990959085399, 'colsample_bytree': 0.5340019271273618, 'reg_alpha': 0.873638443978768, 'reg_lambda': 8.287972964633143, 'min_child_weight': 5, 'gamma': 0.6131020982641403}. Best is trial 94 with value: 1.0230013178475659.\n",
      "[I 2024-08-12 16:16:45,229] Trial 98 finished with value: 1.031469859466063 and parameters: {'n_estimators': 550, 'max_depth': 11, 'learning_rate': 0.033892198414376636, 'subsample': 0.8543354684944066, 'colsample_bytree': 0.553309646151027, 'reg_alpha': 0.5094261569714654, 'reg_lambda': 7.582917549925816, 'min_child_weight': 6, 'gamma': 1.3990887229181628}. Best is trial 94 with value: 1.0230013178475659.\n",
      "[I 2024-08-12 16:16:48,393] Trial 99 finished with value: 1.0257546599668008 and parameters: {'n_estimators': 554, 'max_depth': 11, 'learning_rate': 0.03776412256837425, 'subsample': 0.8538572801560987, 'colsample_bytree': 0.5489836057430989, 'reg_alpha': 0.5729519755166689, 'reg_lambda': 8.502928459445574, 'min_child_weight': 6, 'gamma': 2.441259444171586}. Best is trial 94 with value: 1.0230013178475659.\n",
      "[I 2024-08-12 16:16:52,003] Trial 100 finished with value: 1.0327510767349195 and parameters: {'n_estimators': 488, 'max_depth': 10, 'learning_rate': 0.01744534435566916, 'subsample': 0.9092163532692994, 'colsample_bytree': 0.623700859161007, 'reg_alpha': 1.161801449754932, 'reg_lambda': 8.594837199314652, 'min_child_weight': 6, 'gamma': 2.6619462880723384}. Best is trial 94 with value: 1.0230013178475659.\n",
      "[I 2024-08-12 16:16:57,514] Trial 101 finished with value: 1.0342592319082669 and parameters: {'n_estimators': 550, 'max_depth': 11, 'learning_rate': 0.010236055593693305, 'subsample': 0.8534224531120673, 'colsample_bytree': 0.5532727315749878, 'reg_alpha': 0.5646404186698673, 'reg_lambda': 8.891793200531977, 'min_child_weight': 6, 'gamma': 1.7623774726381947}. Best is trial 94 with value: 1.0230013178475659.\n",
      "[I 2024-08-12 16:17:01,088] Trial 102 finished with value: 1.0336329945000229 and parameters: {'n_estimators': 550, 'max_depth': 12, 'learning_rate': 0.03976518236195879, 'subsample': 0.8607035254937329, 'colsample_bytree': 0.5630821817917417, 'reg_alpha': 0.4016449941684506, 'reg_lambda': 7.631586815480525, 'min_child_weight': 6, 'gamma': 1.8892174460111484}. Best is trial 94 with value: 1.0230013178475659.\n",
      "[I 2024-08-12 16:17:04,394] Trial 103 finished with value: 1.029243073842833 and parameters: {'n_estimators': 463, 'max_depth': 11, 'learning_rate': 0.03229898472359266, 'subsample': 0.9322365704159455, 'colsample_bytree': 0.5432985619852725, 'reg_alpha': 0.23232506087629612, 'reg_lambda': 8.497913216776025, 'min_child_weight': 5, 'gamma': 2.4656040993853217}. Best is trial 94 with value: 1.0230013178475659.\n",
      "[W 2024-08-12 16:17:06,437] Trial 104 failed with parameters: {'n_estimators': 468, 'max_depth': 10, 'learning_rate': 0.06349971063692228, 'subsample': 0.873744594835112, 'colsample_bytree': 0.5415859132384631, 'reg_alpha': 0.27110892194348435, 'reg_lambda': 8.434600898640474, 'min_child_weight': 5, 'gamma': 2.4516836483699165} because of the following error: KeyboardInterrupt().\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\ProgramData\\anaconda3\\Lib\\site-packages\\optuna\\study\\_optimize.py\", line 196, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "                      ^^^^^^^^^^^\n",
      "  File \"C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_9428\\3961053611.py\", line 32, in objective_mse\n",
      "    cv_scores = cross_val_score(xgb_model, X_train, y_train.values.ravel(), cv=5, scoring='neg_mean_squared_error')\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 515, in cross_val_score\n",
      "    cv_results = cross_validate(\n",
      "                 ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 266, in cross_validate\n",
      "    results = parallel(\n",
      "              ^^^^^^^^^\n",
      "  File \"c:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\parallel.py\", line 63, in __call__\n",
      "    return super().__call__(iterable_with_config)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\ProgramData\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py\", line 1088, in __call__\n",
      "    while self.dispatch_one_batch(iterator):\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\ProgramData\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py\", line 901, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"c:\\ProgramData\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py\", line 819, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\ProgramData\\anaconda3\\Lib\\site-packages\\joblib\\_parallel_backends.py\", line 208, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\ProgramData\\anaconda3\\Lib\\site-packages\\joblib\\_parallel_backends.py\", line 597, in __init__\n",
      "    self.results = batch()\n",
      "                   ^^^^^^^\n",
      "  File \"c:\\ProgramData\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py\", line 288, in __call__\n",
      "    return [func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\ProgramData\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py\", line 288, in <listcomp>\n",
      "    return [func(*args, **kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\parallel.py\", line 123, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\ProgramData\\anaconda3\\Lib\\site-packages\\xgboost\\core.py\", line 726, in inner_f\n",
      "    return func(**kwargs)\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"c:\\ProgramData\\anaconda3\\Lib\\site-packages\\xgboost\\sklearn.py\", line 1108, in fit\n",
      "    self._Booster = train(\n",
      "                    ^^^^^^\n",
      "  File \"c:\\ProgramData\\anaconda3\\Lib\\site-packages\\xgboost\\core.py\", line 726, in inner_f\n",
      "    return func(**kwargs)\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"c:\\ProgramData\\anaconda3\\Lib\\site-packages\\xgboost\\training.py\", line 181, in train\n",
      "    bst.update(dtrain, iteration=i, fobj=obj)\n",
      "  File \"c:\\ProgramData\\anaconda3\\Lib\\site-packages\\xgboost\\core.py\", line 2101, in update\n",
      "    _LIB.XGBoosterUpdateOneIter(\n",
      "KeyboardInterrupt\n",
      "[W 2024-08-12 16:17:06,440] Trial 104 failed with value None.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\User\\OneDrive\\Desktop\\PythonNotebooks\\Portugal_Grades1.ipynb Cell 11\u001b[0m line \u001b[0;36m6\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/User/OneDrive/Desktop/PythonNotebooks/Portugal_Grades1.ipynb#X13sZmlsZQ%3D%3D?line=58'>59</a>\u001b[0m \u001b[39m#create study and optimize\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/User/OneDrive/Desktop/PythonNotebooks/Portugal_Grades1.ipynb#X13sZmlsZQ%3D%3D?line=59'>60</a>\u001b[0m study_mse \u001b[39m=\u001b[39m optuna\u001b[39m.\u001b[39mcreate_study(direction\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mminimize\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/User/OneDrive/Desktop/PythonNotebooks/Portugal_Grades1.ipynb#X13sZmlsZQ%3D%3D?line=60'>61</a>\u001b[0m study_mse\u001b[39m.\u001b[39moptimize(objective_mse, n_trials\u001b[39m=\u001b[39m\u001b[39m300\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/User/OneDrive/Desktop/PythonNotebooks/Portugal_Grades1.ipynb#X13sZmlsZQ%3D%3D?line=62'>63</a>\u001b[0m \u001b[39m#study_mae = optuna.create_study(direction='minimize')\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/User/OneDrive/Desktop/PythonNotebooks/Portugal_Grades1.ipynb#X13sZmlsZQ%3D%3D?line=63'>64</a>\u001b[0m \u001b[39m#study_mae.optimize(objective_mae, n_trials=250)\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/User/OneDrive/Desktop/PythonNotebooks/Portugal_Grades1.ipynb#X13sZmlsZQ%3D%3D?line=64'>65</a>\u001b[0m \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/User/OneDrive/Desktop/PythonNotebooks/Portugal_Grades1.ipynb#X13sZmlsZQ%3D%3D?line=65'>66</a>\u001b[0m \u001b[39m#obtain best params + train final model\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/User/OneDrive/Desktop/PythonNotebooks/Portugal_Grades1.ipynb#X13sZmlsZQ%3D%3D?line=66'>67</a>\u001b[0m best_params_mse \u001b[39m=\u001b[39m study_mse\u001b[39m.\u001b[39mbest_params\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\optuna\\study\\study.py:451\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[1;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m    348\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39moptimize\u001b[39m(\n\u001b[0;32m    349\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    350\u001b[0m     func: ObjectiveFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    357\u001b[0m     show_progress_bar: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m    358\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    359\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[0;32m    360\u001b[0m \n\u001b[0;32m    361\u001b[0m \u001b[39m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    449\u001b[0m \u001b[39m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[0;32m    450\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 451\u001b[0m     _optimize(\n\u001b[0;32m    452\u001b[0m         study\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m,\n\u001b[0;32m    453\u001b[0m         func\u001b[39m=\u001b[39mfunc,\n\u001b[0;32m    454\u001b[0m         n_trials\u001b[39m=\u001b[39mn_trials,\n\u001b[0;32m    455\u001b[0m         timeout\u001b[39m=\u001b[39mtimeout,\n\u001b[0;32m    456\u001b[0m         n_jobs\u001b[39m=\u001b[39mn_jobs,\n\u001b[0;32m    457\u001b[0m         catch\u001b[39m=\u001b[39m\u001b[39mtuple\u001b[39m(catch) \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(catch, Iterable) \u001b[39melse\u001b[39;00m (catch,),\n\u001b[0;32m    458\u001b[0m         callbacks\u001b[39m=\u001b[39mcallbacks,\n\u001b[0;32m    459\u001b[0m         gc_after_trial\u001b[39m=\u001b[39mgc_after_trial,\n\u001b[0;32m    460\u001b[0m         show_progress_bar\u001b[39m=\u001b[39mshow_progress_bar,\n\u001b[0;32m    461\u001b[0m     )\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\optuna\\study\\_optimize.py:62\u001b[0m, in \u001b[0;36m_optimize\u001b[1;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     61\u001b[0m     \u001b[39mif\u001b[39;00m n_jobs \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m---> 62\u001b[0m         _optimize_sequential(\n\u001b[0;32m     63\u001b[0m             study,\n\u001b[0;32m     64\u001b[0m             func,\n\u001b[0;32m     65\u001b[0m             n_trials,\n\u001b[0;32m     66\u001b[0m             timeout,\n\u001b[0;32m     67\u001b[0m             catch,\n\u001b[0;32m     68\u001b[0m             callbacks,\n\u001b[0;32m     69\u001b[0m             gc_after_trial,\n\u001b[0;32m     70\u001b[0m             reseed_sampler_rng\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m     71\u001b[0m             time_start\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[0;32m     72\u001b[0m             progress_bar\u001b[39m=\u001b[39mprogress_bar,\n\u001b[0;32m     73\u001b[0m         )\n\u001b[0;32m     74\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     75\u001b[0m         \u001b[39mif\u001b[39;00m n_jobs \u001b[39m==\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m:\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\optuna\\study\\_optimize.py:159\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[1;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[0;32m    156\u001b[0m         \u001b[39mbreak\u001b[39;00m\n\u001b[0;32m    158\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 159\u001b[0m     frozen_trial \u001b[39m=\u001b[39m _run_trial(study, func, catch)\n\u001b[0;32m    160\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m    161\u001b[0m     \u001b[39m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[0;32m    162\u001b[0m     \u001b[39m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[0;32m    163\u001b[0m     \u001b[39m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[0;32m    164\u001b[0m     \u001b[39m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[0;32m    165\u001b[0m     \u001b[39mif\u001b[39;00m gc_after_trial:\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\optuna\\study\\_optimize.py:247\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    240\u001b[0m         \u001b[39massert\u001b[39;00m \u001b[39mFalse\u001b[39;00m, \u001b[39m\"\u001b[39m\u001b[39mShould not reach.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    242\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[0;32m    243\u001b[0m     frozen_trial\u001b[39m.\u001b[39mstate \u001b[39m==\u001b[39m TrialState\u001b[39m.\u001b[39mFAIL\n\u001b[0;32m    244\u001b[0m     \u001b[39mand\u001b[39;00m func_err \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    245\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(func_err, catch)\n\u001b[0;32m    246\u001b[0m ):\n\u001b[1;32m--> 247\u001b[0m     \u001b[39mraise\u001b[39;00m func_err\n\u001b[0;32m    248\u001b[0m \u001b[39mreturn\u001b[39;00m frozen_trial\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\optuna\\study\\_optimize.py:196\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    194\u001b[0m \u001b[39mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[39m.\u001b[39m_trial_id, study\u001b[39m.\u001b[39m_storage):\n\u001b[0;32m    195\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 196\u001b[0m         value_or_values \u001b[39m=\u001b[39m func(trial)\n\u001b[0;32m    197\u001b[0m     \u001b[39mexcept\u001b[39;00m exceptions\u001b[39m.\u001b[39mTrialPruned \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    198\u001b[0m         \u001b[39m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[0;32m    199\u001b[0m         state \u001b[39m=\u001b[39m TrialState\u001b[39m.\u001b[39mPRUNED\n",
      "\u001b[1;32mc:\\Users\\User\\OneDrive\\Desktop\\PythonNotebooks\\Portugal_Grades1.ipynb Cell 11\u001b[0m line \u001b[0;36m3\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/User/OneDrive/Desktop/PythonNotebooks/Portugal_Grades1.ipynb#X13sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m xgb_model \u001b[39m=\u001b[39m xgb\u001b[39m.\u001b[39mXGBRegressor(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparam)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/User/OneDrive/Desktop/PythonNotebooks/Portugal_Grades1.ipynb#X13sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m \u001b[39m#use cross validation for stabler evaluation\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/User/OneDrive/Desktop/PythonNotebooks/Portugal_Grades1.ipynb#X13sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m cv_scores \u001b[39m=\u001b[39m cross_val_score(xgb_model, X_train, y_train\u001b[39m.\u001b[39mvalues\u001b[39m.\u001b[39mravel(), cv\u001b[39m=\u001b[39m\u001b[39m5\u001b[39m, scoring\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mneg_mean_squared_error\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/User/OneDrive/Desktop/PythonNotebooks/Portugal_Grades1.ipynb#X13sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m \u001b[39m*\u001b[39m cv_scores\u001b[39m.\u001b[39mmean()\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:515\u001b[0m, in \u001b[0;36mcross_val_score\u001b[1;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, error_score)\u001b[0m\n\u001b[0;32m    512\u001b[0m \u001b[39m# To ensure multimetric format is not supported\u001b[39;00m\n\u001b[0;32m    513\u001b[0m scorer \u001b[39m=\u001b[39m check_scoring(estimator, scoring\u001b[39m=\u001b[39mscoring)\n\u001b[1;32m--> 515\u001b[0m cv_results \u001b[39m=\u001b[39m cross_validate(\n\u001b[0;32m    516\u001b[0m     estimator\u001b[39m=\u001b[39mestimator,\n\u001b[0;32m    517\u001b[0m     X\u001b[39m=\u001b[39mX,\n\u001b[0;32m    518\u001b[0m     y\u001b[39m=\u001b[39my,\n\u001b[0;32m    519\u001b[0m     groups\u001b[39m=\u001b[39mgroups,\n\u001b[0;32m    520\u001b[0m     scoring\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39mscore\u001b[39m\u001b[39m\"\u001b[39m: scorer},\n\u001b[0;32m    521\u001b[0m     cv\u001b[39m=\u001b[39mcv,\n\u001b[0;32m    522\u001b[0m     n_jobs\u001b[39m=\u001b[39mn_jobs,\n\u001b[0;32m    523\u001b[0m     verbose\u001b[39m=\u001b[39mverbose,\n\u001b[0;32m    524\u001b[0m     fit_params\u001b[39m=\u001b[39mfit_params,\n\u001b[0;32m    525\u001b[0m     pre_dispatch\u001b[39m=\u001b[39mpre_dispatch,\n\u001b[0;32m    526\u001b[0m     error_score\u001b[39m=\u001b[39merror_score,\n\u001b[0;32m    527\u001b[0m )\n\u001b[0;32m    528\u001b[0m \u001b[39mreturn\u001b[39;00m cv_results[\u001b[39m\"\u001b[39m\u001b[39mtest_score\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:266\u001b[0m, in \u001b[0;36mcross_validate\u001b[1;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, return_train_score, return_estimator, error_score)\u001b[0m\n\u001b[0;32m    263\u001b[0m \u001b[39m# We clone the estimator to make sure that all the folds are\u001b[39;00m\n\u001b[0;32m    264\u001b[0m \u001b[39m# independent, and that it is pickle-able.\u001b[39;00m\n\u001b[0;32m    265\u001b[0m parallel \u001b[39m=\u001b[39m Parallel(n_jobs\u001b[39m=\u001b[39mn_jobs, verbose\u001b[39m=\u001b[39mverbose, pre_dispatch\u001b[39m=\u001b[39mpre_dispatch)\n\u001b[1;32m--> 266\u001b[0m results \u001b[39m=\u001b[39m parallel(\n\u001b[0;32m    267\u001b[0m     delayed(_fit_and_score)(\n\u001b[0;32m    268\u001b[0m         clone(estimator),\n\u001b[0;32m    269\u001b[0m         X,\n\u001b[0;32m    270\u001b[0m         y,\n\u001b[0;32m    271\u001b[0m         scorers,\n\u001b[0;32m    272\u001b[0m         train,\n\u001b[0;32m    273\u001b[0m         test,\n\u001b[0;32m    274\u001b[0m         verbose,\n\u001b[0;32m    275\u001b[0m         \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m    276\u001b[0m         fit_params,\n\u001b[0;32m    277\u001b[0m         return_train_score\u001b[39m=\u001b[39mreturn_train_score,\n\u001b[0;32m    278\u001b[0m         return_times\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[0;32m    279\u001b[0m         return_estimator\u001b[39m=\u001b[39mreturn_estimator,\n\u001b[0;32m    280\u001b[0m         error_score\u001b[39m=\u001b[39merror_score,\n\u001b[0;32m    281\u001b[0m     )\n\u001b[0;32m    282\u001b[0m     \u001b[39mfor\u001b[39;00m train, test \u001b[39min\u001b[39;00m cv\u001b[39m.\u001b[39msplit(X, y, groups)\n\u001b[0;32m    283\u001b[0m )\n\u001b[0;32m    285\u001b[0m _warn_or_raise_about_fit_failures(results, error_score)\n\u001b[0;32m    287\u001b[0m \u001b[39m# For callabe scoring, the return type is only know after calling. If the\u001b[39;00m\n\u001b[0;32m    288\u001b[0m \u001b[39m# return type is a dictionary, the error scores can now be inserted with\u001b[39;00m\n\u001b[0;32m    289\u001b[0m \u001b[39m# the correct key.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\parallel.py:63\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     58\u001b[0m config \u001b[39m=\u001b[39m get_config()\n\u001b[0;32m     59\u001b[0m iterable_with_config \u001b[39m=\u001b[39m (\n\u001b[0;32m     60\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     61\u001b[0m     \u001b[39mfor\u001b[39;00m delayed_func, args, kwargs \u001b[39min\u001b[39;00m iterable\n\u001b[0;32m     62\u001b[0m )\n\u001b[1;32m---> 63\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__call__\u001b[39m(iterable_with_config)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py:1088\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1085\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdispatch_one_batch(iterator):\n\u001b[0;32m   1086\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterating \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_original_iterator \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m-> 1088\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdispatch_one_batch(iterator):\n\u001b[0;32m   1089\u001b[0m     \u001b[39mpass\u001b[39;00m\n\u001b[0;32m   1091\u001b[0m \u001b[39mif\u001b[39;00m pre_dispatch \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mall\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mor\u001b[39;00m n_jobs \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m   1092\u001b[0m     \u001b[39m# The iterable was consumed all at once by the above for loop.\u001b[39;00m\n\u001b[0;32m   1093\u001b[0m     \u001b[39m# No need to wait for async callbacks to trigger to\u001b[39;00m\n\u001b[0;32m   1094\u001b[0m     \u001b[39m# consumption.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py:901\u001b[0m, in \u001b[0;36mParallel.dispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    899\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m    900\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 901\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dispatch(tasks)\n\u001b[0;32m    902\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py:819\u001b[0m, in \u001b[0;36mParallel._dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    817\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m    818\u001b[0m     job_idx \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jobs)\n\u001b[1;32m--> 819\u001b[0m     job \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend\u001b[39m.\u001b[39mapply_async(batch, callback\u001b[39m=\u001b[39mcb)\n\u001b[0;32m    820\u001b[0m     \u001b[39m# A job can complete so quickly than its callback is\u001b[39;00m\n\u001b[0;32m    821\u001b[0m     \u001b[39m# called before we get here, causing self._jobs to\u001b[39;00m\n\u001b[0;32m    822\u001b[0m     \u001b[39m# grow. To ensure correct results ordering, .insert is\u001b[39;00m\n\u001b[0;32m    823\u001b[0m     \u001b[39m# used (rather than .append) in the following line\u001b[39;00m\n\u001b[0;32m    824\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jobs\u001b[39m.\u001b[39minsert(job_idx, job)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\joblib\\_parallel_backends.py:208\u001b[0m, in \u001b[0;36mSequentialBackend.apply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    206\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply_async\u001b[39m(\u001b[39mself\u001b[39m, func, callback\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m    207\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Schedule a func to be run\"\"\"\u001b[39;00m\n\u001b[1;32m--> 208\u001b[0m     result \u001b[39m=\u001b[39m ImmediateResult(func)\n\u001b[0;32m    209\u001b[0m     \u001b[39mif\u001b[39;00m callback:\n\u001b[0;32m    210\u001b[0m         callback(result)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\joblib\\_parallel_backends.py:597\u001b[0m, in \u001b[0;36mImmediateResult.__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    594\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, batch):\n\u001b[0;32m    595\u001b[0m     \u001b[39m# Don't delay the application, to avoid keeping the input\u001b[39;00m\n\u001b[0;32m    596\u001b[0m     \u001b[39m# arguments in memory\u001b[39;00m\n\u001b[1;32m--> 597\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresults \u001b[39m=\u001b[39m batch()\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py:288\u001b[0m, in \u001b[0;36mBatchedCalls.__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    285\u001b[0m     \u001b[39m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[0;32m    286\u001b[0m     \u001b[39m# change the default number of processes to -1\u001b[39;00m\n\u001b[0;32m    287\u001b[0m     \u001b[39mwith\u001b[39;00m parallel_backend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend, n_jobs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_n_jobs):\n\u001b[1;32m--> 288\u001b[0m         \u001b[39mreturn\u001b[39;00m [func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    289\u001b[0m                 \u001b[39mfor\u001b[39;00m func, args, kwargs \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitems]\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py:288\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    285\u001b[0m     \u001b[39m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[0;32m    286\u001b[0m     \u001b[39m# change the default number of processes to -1\u001b[39;00m\n\u001b[0;32m    287\u001b[0m     \u001b[39mwith\u001b[39;00m parallel_backend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend, n_jobs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_n_jobs):\n\u001b[1;32m--> 288\u001b[0m         \u001b[39mreturn\u001b[39;00m [func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    289\u001b[0m                 \u001b[39mfor\u001b[39;00m func, args, kwargs \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitems]\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\parallel.py:123\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    121\u001b[0m     config \u001b[39m=\u001b[39m {}\n\u001b[0;32m    122\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mconfig):\n\u001b[1;32m--> 123\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfunction(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:686\u001b[0m, in \u001b[0;36m_fit_and_score\u001b[1;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001b[0m\n\u001b[0;32m    684\u001b[0m         estimator\u001b[39m.\u001b[39mfit(X_train, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params)\n\u001b[0;32m    685\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 686\u001b[0m         estimator\u001b[39m.\u001b[39mfit(X_train, y_train, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params)\n\u001b[0;32m    688\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:\n\u001b[0;32m    689\u001b[0m     \u001b[39m# Note fit time as time until error\u001b[39;00m\n\u001b[0;32m    690\u001b[0m     fit_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m start_time\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:726\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    724\u001b[0m \u001b[39mfor\u001b[39;00m k, arg \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(sig\u001b[39m.\u001b[39mparameters, args):\n\u001b[0;32m    725\u001b[0m     kwargs[k] \u001b[39m=\u001b[39m arg\n\u001b[1;32m--> 726\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\xgboost\\sklearn.py:1108\u001b[0m, in \u001b[0;36mXGBModel.fit\u001b[1;34m(self, X, y, sample_weight, base_margin, eval_set, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights)\u001b[0m\n\u001b[0;32m   1105\u001b[0m     obj \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m   1107\u001b[0m model, metric, params \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_configure_fit(xgb_model, params)\n\u001b[1;32m-> 1108\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_Booster \u001b[39m=\u001b[39m train(\n\u001b[0;32m   1109\u001b[0m     params,\n\u001b[0;32m   1110\u001b[0m     train_dmatrix,\n\u001b[0;32m   1111\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_num_boosting_rounds(),\n\u001b[0;32m   1112\u001b[0m     evals\u001b[39m=\u001b[39mevals,\n\u001b[0;32m   1113\u001b[0m     early_stopping_rounds\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mearly_stopping_rounds,\n\u001b[0;32m   1114\u001b[0m     evals_result\u001b[39m=\u001b[39mevals_result,\n\u001b[0;32m   1115\u001b[0m     obj\u001b[39m=\u001b[39mobj,\n\u001b[0;32m   1116\u001b[0m     custom_metric\u001b[39m=\u001b[39mmetric,\n\u001b[0;32m   1117\u001b[0m     verbose_eval\u001b[39m=\u001b[39mverbose,\n\u001b[0;32m   1118\u001b[0m     xgb_model\u001b[39m=\u001b[39mmodel,\n\u001b[0;32m   1119\u001b[0m     callbacks\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallbacks,\n\u001b[0;32m   1120\u001b[0m )\n\u001b[0;32m   1122\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_set_evaluation_result(evals_result)\n\u001b[0;32m   1123\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:726\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    724\u001b[0m \u001b[39mfor\u001b[39;00m k, arg \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(sig\u001b[39m.\u001b[39mparameters, args):\n\u001b[0;32m    725\u001b[0m     kwargs[k] \u001b[39m=\u001b[39m arg\n\u001b[1;32m--> 726\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\xgboost\\training.py:181\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, custom_metric)\u001b[0m\n\u001b[0;32m    179\u001b[0m \u001b[39mif\u001b[39;00m cb_container\u001b[39m.\u001b[39mbefore_iteration(bst, i, dtrain, evals):\n\u001b[0;32m    180\u001b[0m     \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m--> 181\u001b[0m bst\u001b[39m.\u001b[39mupdate(dtrain, iteration\u001b[39m=\u001b[39mi, fobj\u001b[39m=\u001b[39mobj)\n\u001b[0;32m    182\u001b[0m \u001b[39mif\u001b[39;00m cb_container\u001b[39m.\u001b[39mafter_iteration(bst, i, dtrain, evals):\n\u001b[0;32m    183\u001b[0m     \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:2101\u001b[0m, in \u001b[0;36mBooster.update\u001b[1;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[0;32m   2097\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_assign_dmatrix_features(dtrain)\n\u001b[0;32m   2099\u001b[0m \u001b[39mif\u001b[39;00m fobj \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   2100\u001b[0m     _check_call(\n\u001b[1;32m-> 2101\u001b[0m         _LIB\u001b[39m.\u001b[39mXGBoosterUpdateOneIter(\n\u001b[0;32m   2102\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandle, ctypes\u001b[39m.\u001b[39mc_int(iteration), dtrain\u001b[39m.\u001b[39mhandle\n\u001b[0;32m   2103\u001b[0m         )\n\u001b[0;32m   2104\u001b[0m     )\n\u001b[0;32m   2105\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   2106\u001b[0m     pred \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpredict(dtrain, output_margin\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, training\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import optuna #hyperparam optimizer(bayesian optimization)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_clean, y_clean, test_size=0.2, random_state=42)\n",
    "y_train = y_train.squeeze()\n",
    "y_test = y_test.squeeze()\n",
    "#make sure features are in correct format\n",
    "X_train = X_train.values if isinstance(X_train, pd.DataFrame) else X_train\n",
    "X_test = X_test.values if isinstance(X_test, pd.DataFrame) else X_test\n",
    "\n",
    "\n",
    "def objective_mse(trial):\n",
    "    param = {\n",
    "        'objective': 'reg:squarederror',\n",
    "        'random_state': 42,\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 100, 1000),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 20),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.5),\n",
    "        'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n",
    "        'reg_alpha': trial.suggest_float('reg_alpha', 0.0, 10),\n",
    "        'reg_lambda': trial.suggest_float('reg_lambda', 0.0, 10),\n",
    "        'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),\n",
    "        'gamma': trial.suggest_float('gamma', 0.0, 5.0),\n",
    "        'tree_method': 'hist',  #use histogram-based method (CPU)\n",
    "        'device': 'cuda'        #use GPU (CUDA) for training\n",
    "    }\n",
    "\n",
    "    #train model with the current set of hyperparameters\n",
    "    xgb_model = xgb.XGBRegressor(**param)\n",
    "\n",
    "    #use cross validation for stabler evaluation\n",
    "    cv_scores = cross_val_score(xgb_model, X_train, y_train.values.ravel(), cv=5, scoring='neg_mean_squared_error')\n",
    "\n",
    "    return -1 * cv_scores.mean()\n",
    "\n",
    "def objective_mae(trial):\n",
    "    param = {\n",
    "        'objective': 'reg:squarederror',\n",
    "        'random_state': 42,\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 100, 1000),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 15),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3),\n",
    "        'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n",
    "        'reg_alpha': trial.suggest_float('reg_alpha', 0.0, 10.0),\n",
    "        'reg_lambda': trial.suggest_float('reg_lambda', 0.0, 10.0),\n",
    "        'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),\n",
    "        'gamma': trial.suggest_float('gamma', 0.0, 5.0),\n",
    "    }\n",
    "\n",
    "    #train model with the current set of hyperparameters\n",
    "    xgb_model = xgb.XGBRegressor(**param)\n",
    "\n",
    "    #use cross validation for stabler evaluation\n",
    "    cv_scores = cross_val_score(xgb_model, X_train, y_train.values.ravel(), cv=5, scoring='neg_mean_absolute_error')\n",
    "\n",
    "    return -1 * cv_scores.mean()\n",
    "\n",
    "#create study and optimize\n",
    "study_mse = optuna.create_study(direction='minimize')\n",
    "study_mse.optimize(objective_mse, n_trials=300)\n",
    "\n",
    "#study_mae = optuna.create_study(direction='minimize')\n",
    "#study_mae.optimize(objective_mae, n_trials=250)\n",
    "\n",
    "#obtain best params + train final model\n",
    "best_params_mse = study_mse.best_params\n",
    "#best_params_mae = study_mae.best_params\n",
    "\n",
    "best_xgb_model_mse = xgb.XGBRegressor(**best_params_mse)\n",
    "best_xgb_model_mse.fit(X_train, y_train.values.ravel())\n",
    "y_pred_final_mse = best_xgb_model_mse.predict(X_test)\n",
    "mse_final = mean_squared_error(y_test, y_pred_final_mse)\n",
    "\n",
    "#best_xgb_model_mae = xgb.XGBRegressor(**best_params_mae)\n",
    "# best_xgb_model_mae.fit(X_train, y_train.values.ravel())\n",
    "# y_pred_final_mae = best_xgb_model_mae.predict(X_test)\n",
    "# mae_final = mean_absolute_error(y_test, y_pred_final_mae)\n",
    "\n",
    "print(f'Final Model Mean Squared Error: {mse_final}')\n",
    "#print(f'Final Model Mean Absolute Error: {mae_final}')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With optuna(bayesian optimization), obtained 1.56 MSE, 0.747 MAE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After removing outliers, obtained 1.37 MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
